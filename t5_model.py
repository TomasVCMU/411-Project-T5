# -*- coding: utf-8 -*-
"""t5-model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g2uWflRmRHX29DMES6AOJrqtPtpp6-Hz
"""

import json
import pandas as pd
from tracemalloc import start
from transformers import AdamW,T5ForConditionalGeneration,T5TokenizerFast as T5Tokenizer
import torch
import tqdm
from torch.utils.data import DataLoader, Dataset
import pytorch_lightning as pl
from keras.callbacks import ModelCheckpoint
import sys

def readData(path):
    with open(path, 'rb') as f:
        data = json.load(f)

    # Finding all the contexts, questions, and answers in the SQUAD dataset
    cntxts = []
    qstns = []
    answrs = []
    answer = []
    astart = []   
    # Preprocessing through iteration of the dataset
    for doc in data['data']:
        for paragraph in doc['paragraphs']:
            cntxt = paragraph['context']
            for qa in paragraph['qas']:
                qstn = qa['question']
                if 'plausible_answers' in qa.keys():
                    toPick = 'plausible_answers'
                else:
                    toPick = 'answers'
                for answr in qa[toPick]:
                    # Appending data to their corresponding lists
                    cntxts.append(cntxt)
                    qstns.append(qstn)
                    answrs.append(answr)
    for ans in answrs:
        answer.append(ans['text'])
        astart.append(ans['answer_start'])   
    return cntxts, qstns, answrs, answer, astart 

# Grabbing the corresponding data in our desired format for training and validation datasets (validation not formatted yet)
# TODO: Add validation data here
trainingContexts, trainingQuestions, trainingAnswerObj, answerText, answerStart = readData('./data/train-v2.0.json')

# Add end index
def addEndIndex(answers, contexts):
    # Zip answers and context into answer-context pairs and loop through to process indeces
    answer_end = []
    for answer, context in zip(answers, contexts):
        desiredText = answer['text']
        startIndex = answer['answer_start']
        endIndex = startIndex + len(desiredText)

        # Squad dataset can be off by one or two characters to the left, so we account for this
        if context[startIndex:endIndex] != desiredText:
            ctr = 1
            while ctr < 3:
                if context[startIndex-ctr:endIndex-ctr] == desiredText:
                    answer['answer_start'] = startIndex - ctr
                    answer['answer_end'] = endIndex - ctr
                    break
                ctr += 1
        else:
            answer['answer_end'] = endIndex
        answer_end.append(endIndex)
    return answer_end

endindex = addEndIndex(trainingAnswerObj, trainingContexts)

#to Dataframe

train_data = pd.DataFrame({'context': trainingContexts,
                           'question': trainingQuestions,
                           'answer': answerText,
                           'answer_start': answerStart,
                           'answer_end': endindex},
                          columns = ['context','question','answer', 'answer_start', 'answer_end'])

train_data.drop_duplicates(subset=['question'], inplace=True)

class SQUADDataset(Dataset):
    def __init__(self, data, tokenizer, source_max_token_len, target_max_token_len):
        self.tokenizer = tokenizer
        self.data = data
        self.source_max_token_len = source_max_token_len
        self.target_max_token_len = target_max_token_len

    def __getitem__(self, idx):
        dataRow = self.data.iloc[idx]
        

        sourceEncoding = tokenizer(
                            dataRow['question'], 
                            dataRow['context'],  
                            padding="max_length",
                            max_length=self.source_max_token_len,
                            truncation=True,
                            return_attention_mask=True,
                            add_special_tokens=True,
                            return_tensors="pt",
        )

        answerEncoding = tokenizer(
                    dataRow['answer'],
                    padding="max_length",
                    max_length=self.target_max_token_len,
                    truncation=True,
                    return_attention_mask=True,
                    add_special_tokens=True,
                    return_tensors="pt",
        )

        labels = answerEncoding['input_ids']
        labels[labels == 0] = -100

        return dict(
            question=dataRow['question'],
            context=dataRow['context'],
            answer_text=dataRow['answer'],
            input_ids=sourceEncoding['input_ids'].flatten(),
            attention_mask=sourceEncoding['input_ids'].flatten(),
            labels=labels.flatten()
        )
    def __len__(self):
        return len(self.data)

class SQUADModule(pl.LightningModule):
    def __init__(self, train_data, test_data, tokenizer, batch_size=8, source_max_token_len=512, target_max_token_len=32):
      super().__init__()
      self.batch_size = batch_size
      self.train_data = train_data
      self.test_data = test_data
      self.tokenizer = tokenizer
      self.source_max_token_len = source_max_token_len
      self.target_max_token_len = target_max_token_len

    def setup(self, stage=None):
      self.train_dataset = SQUADDataset(
          self.train_data,
          self.tokenizer,
          self.source_max_token_len,
          self.target_max_token_len
      )

      self.val_dataset = SQUADDataset(
          self.train_data, #change to val dataset
          self.tokenizer,
          self.source_max_token_len,
          self.target_max_token_len
      )
    
    def train_dataloader(self):
      return DataLoader(
          self.train_dataset,
          batch_size=self.batch_size,
          shuffle=True,
          num_workers=4
      )
    
    def val_dataloader(self):
      return DataLoader(
          self.val_dataset,
          batch_size=1,
          shuffle=True,
          num_workers=4
      )

BATCH_SIZE = 8
NUM_EPOCH = 6

MODEL_NAME = 't5-base'
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

SQUADDataset(train_data, tokenizer, 512, 32)

class SQUADModel(pl.LightningModule):
    
    def __init__(self):
        super().__init__()
        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict = True)
    
    def forward(self, input_ids, attention_mask, labels=None):
        output = self.model(
            input_ids = input_ids,
            attention_mask = attention_mask,
            labels = labels
            )
        return output.loss, output.logits
    
    def training_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]
        loss, outputs = self.forward(input_ids, attention_mask, labels)
        print("LOG: ", input_ids, attention_mask, labels)
        self.log("train_loss", loss, prog_bar=True, logger=True)
        return loss
    
    def validation_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]
        loss, outputs = self.forward(input_ids, attention_mask, labels)
        self.log("val_loss", loss, prog_bar=True, logger=True)
        return loss
    
    def test_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]
        loss, outputs = self.foward(input_ids, attention_mask, labels)
        self.log("test_loss", loss, prog_bar=True, logger=True)
        return loss

    def configure_optimizers(self):
       return AdamW(self.parameters(), lr = 0.001)

dataModule = SQUADModule(train_data, train_data, tokenizer, batch_size=BATCH_SIZE)
dataModule.setup()
model = SQUADModel()
sys.setrecursionlimit(10000)

checkpoint_callback = ModelCheckpoint(
    filepath = 'checkpoints',
    filename='checkpoint',
    save_top_k=1,
    verbose=True,
    monitor='val_loss',
    mode='min'
    )


trainer = pl.Trainer(
    max_epochs=NUM_EPOCH, accelerator="gpu", devices=1
    )

trainer.fit(model, datamodule=dataModule)